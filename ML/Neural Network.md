A neural network is a parameter function for function approximation that, according to the [[Universal Approximation Theorem]], can theoretically approximate any function to any degree of accuracy, depending on the architecture. It sequentially applies linear mappings and non-linear activation functions to the function arguments in a process called [[Neural Network#Forward-propagation |Forward-Propagation]] , where the linear weights are to be learned. They are then typically adjusted through gradient- or hessian-based methods through non-convex optimization methods in a process called [[Neural Network#Backwards-Propagation|Backwards-Propagation]] in order to minimize some objective function, usually a distance function between the desired and current outputs of the model.
## Architecture:
A standard feed-forward neural network consists of three main parts:
1. An input layer, where Inputs get fed in
2. A number of hidden layers with a variable number of neurons
3. An output layer, where projected labels are produced

Where the number of hidden layers as well as their neuron count largely determine the model complexity.
## Forward-propagation:
In forward propagation, the output is produced from the input in the fol


## Backwards-Propagation

