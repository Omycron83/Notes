Entropy is a measure of information or 'suprise' inherent to the potential values of a random variable. 
This means that, if a highly likely event occurs, the entropy of the random variable is low, as it carries low information. If contrary, an event with low likelyhood occurs, the entropy is high.

For example, knowing that a certain number **isn't** the winning number in a lottery carries low information. However, knowing that **it is** the winning number carries a lot of information.
The possible outcome of getting any one side of a die 
## Derivation
This means that the entropy of an event $E$ should be inversely correlated to the probability of an event, and, if the probability of the event is one, it should have an entropy of zero.
A term satisfying this is: $log(\frac{1}{P(E)}) = -log(P(E))$. Usually 2 is chosen as a base for the logarithm.

